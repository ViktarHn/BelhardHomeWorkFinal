{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e525c0-4ee3-42fd-aa6d-bd8ada4ca98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('data_loading.log'), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataLoader:\n",
    "    CONFIDENTIAL_FLAG = 'C'\n",
    "    EXPECTED_COLUMNS = ['geo', 'TIME_PERIOD', 'OBS_VALUE', 'CONF_STATUS']\n",
    "    DEFAULT_CONFIG = {\n",
    "        'encoding': 'utf-8',\n",
    "        'date_columns': ['TIME_PERIOD'],\n",
    "        'source': 'local',\n",
    "        'handle_confidential': True\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"Initialize DataLoader with configuration.\"\"\"\n",
    "        self.df: Optional[pd.DataFrame] = None\n",
    "        self.config = {**self.DEFAULT_CONFIG, **(config or {})}\n",
    "    \n",
    "    def validate_data_structure(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Validate DataFrame structure against expected columns.\"\"\"\n",
    "        missing_columns = set(self.EXPECTED_COLUMNS) - set(df.columns)\n",
    "        if missing_columns:\n",
    "            logger.error(f\"Missing expected columns: {missing_columns}\")\n",
    "            return False\n",
    "        if not pd.api.types.is_numeric_dtype(df['OBS_VALUE']):\n",
    "            logger.warning(\"OBS_VALUE contains non-numeric values\")\n",
    "        duplicates = df.duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            logger.warning(f\"Found {duplicates} duplicate rows\")\n",
    "            df.drop_duplicates(inplace=True)\n",
    "        return True\n",
    "    \n",
    "    def handle_confidential_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Replace confidential values with NaN and interpolate.\"\"\"\n",
    "        if 'CONF_STATUS' in df.columns and self.config.get('handle_confidential', True):\n",
    "            confidential_mask = df['CONF_STATUS'] == self.CONFIDENTIAL_FLAG\n",
    "            n_confidential = confidential_mask.sum()\n",
    "            df.loc[confidential_mask, 'OBS_VALUE'] = np.nan\n",
    "            logger.info(f\"Handled {n_confidential} confidential values (replaced with NaN)\")\n",
    "            df['OBS_VALUE'] = df['OBS_VALUE'].interpolate(method='linear').ffill().bfill()\n",
    "            if df['OBS_VALUE'].isna().all():\n",
    "                logger.warning(\"All OBS_VALUE are NaN after interpolation; filling with 0\")\n",
    "                df['OBS_VALUE'] = df['OBS_VALUE'].fillna(0)\n",
    "        return df\n",
    "    \n",
    "    def check_data_quality(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Log basic data quality metrics.\"\"\"\n",
    "        missing_values = df['OBS_VALUE'].isna().sum()\n",
    "        logger.info(f\"Found {missing_values} missing values in OBS_VALUE after processing\")\n",
    "        min_year = df['TIME_PERIOD'].min()\n",
    "        max_year = df['TIME_PERIOD'].max()\n",
    "        logger.info(f\"Data covers years: {min_year} to {max_year}\")\n",
    "        countries = df['geo'].nunique()\n",
    "        logger.info(f\"Data contains {countries} unique countries\")\n",
    "        if len(df) < 10:\n",
    "            logger.warning(f\"Dataset has only {len(df)} rows, which may be insufficient\")\n",
    "    \n",
    "    def load_data(self, file_path: str = 'estat_tec00107_filtered_en.csv') -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load and preprocess data from file.\"\"\"\n",
    "        try:\n",
    "            if self.config['source'] == 'url':\n",
    "                logger.info(f\"Downloading data from URL: {file_path}\")\n",
    "                local_path = os.path.basename(file_path)\n",
    "                urlretrieve(file_path, local_path)\n",
    "                file_path = local_path\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"File {file_path} not found\")\n",
    "                \n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=self.config['encoding'])\n",
    "            except UnicodeDecodeError:\n",
    "                logger.warning(f\"Failed with encoding {self.config['encoding']}, trying latin-1\")\n",
    "                df = pd.read_csv(file_path, encoding='latin-1')\n",
    "            \n",
    "            for col in self.config['date_columns']:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_datetime(df[col], format='%Y', errors='coerce')\n",
    "            \n",
    "            if not self.validate_data_structure(df):\n",
    "                logger.error(\"Data structure validation failed\")\n",
    "                return None\n",
    "                \n",
    "            df = self.handle_confidential_data(df)\n",
    "            df = df.dropna(how='all', axis=1)  # Удаление полностью пустых столбцов\n",
    "            self.check_data_quality(df)\n",
    "            self.df = df\n",
    "            logger.info(f\"Data loaded successfully: {len(df)} rows\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load data: {str(e)}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "def load_data(file_path: str = 'estat_tec00107_filtered_en.csv', config: Optional[Dict[str, Any]] = None) -> pd.DataFrame:\n",
    "    \"\"\"Wrapper function to load data.\"\"\"\n",
    "    loader = DataLoader(config=config)\n",
    "    df = loader.load_data(file_path)\n",
    "    if df is None or df.empty:\n",
    "        logger.warning(\"No data loaded or empty DataFrame returned\")\n",
    "        return pd.DataFrame()\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"=== Загрузка данных FDI ===\")\n",
    "    df = load_data(\"estat_tec00107_filtered_en.csv\")\n",
    "    if not df.empty:\n",
    "        logger.info(f\"Loaded {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7758ba2-16ee-4185-a9ae-fc30946e6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional\n",
    "from scipy import stats\n",
    "\n",
    "logger = logging.getLogger('eda')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "fh = RotatingFileHandler('eda.log', maxBytes=1e6, backupCount=3)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "class EDAAnalyzer:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        \"\"\"Initialize EDA analyzer with DataFrame.\"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.results: Dict[str, Any] = {\n",
    "            'metadata': {'analysis_date': datetime.now().isoformat(), 'data_source': 'estat_tec00107_filtered_en.csv', 'stats': {}},\n",
    "            'missing_values': {},\n",
    "            'outliers': {},\n",
    "            'correlations': {},\n",
    "            'trends': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "    \n",
    "    def describe_columns(self) -> None:\n",
    "        \"\"\"Describe dataset columns.\"\"\"\n",
    "        logger.info(\"Describing dataset columns\")\n",
    "        print(\"\\n=== Dataset Columns Description ===\")\n",
    "        for col in self.df.columns:\n",
    "            dtype = self.df[col].dtype\n",
    "            unique = self.df[col].nunique()\n",
    "            missing = self.df[col].isna().sum()\n",
    "            print(f\"{col}: Type={dtype}, Unique={unique}, Missing={missing}\")\n",
    "            logger.info(f\"{col}: Type={dtype}, Unique={unique}, Missing={missing}\")\n",
    "    \n",
    "    def _save_plot(self, fig: go.Figure, filename: str) -> None:\n",
    "        \"\"\"Save and display plot.\"\"\"\n",
    "        os.makedirs('plots', exist_ok=True)\n",
    "        path = f\"plots/{filename}.html\"\n",
    "        fig.write_html(path)\n",
    "        fig.show()\n",
    "        logger.info(f\"Plot saved to {path}\")\n",
    "    \n",
    "    def _add_recommendation(self, message: str) -> None:\n",
    "        \"\"\"Add recommendation to results.\"\"\"\n",
    "        self.results['recommendations'].append(message)\n",
    "        logger.info(f\"Recommendation: {message}\")\n",
    "    \n",
    "    def _safe_describe(self, series: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"Safely compute descriptive statistics.\"\"\"\n",
    "        try:\n",
    "            return {\n",
    "                'count': series.count(), 'mean': series.mean(), 'std': series.std(),\n",
    "                'min': series.min(), '25%': series.quantile(0.25), '50%': series.median(),\n",
    "                '75%': series.quantile(0.75), 'max': series.max()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing stats: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def basic_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Compute basic statistics.\"\"\"\n",
    "        logger.info(\"Analyzing basic statistics...\")\n",
    "        self.results['metadata'].update({\n",
    "            'total_rows': len(self.df), 'total_columns': len(self.df.columns),\n",
    "            'time_period': {'start': str(self.df['TIME_PERIOD'].min()), 'end': str(self.df['TIME_PERIOD'].max())},\n",
    "            'unique_countries': self.df['geo'].nunique()\n",
    "        })\n",
    "        num_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        for col in num_cols:\n",
    "            self.results['metadata']['stats'][col] = self._safe_describe(self.df[col])\n",
    "            if self.df[col].isnull().all():\n",
    "                self._add_recommendation(f\"Column {col} is completely empty - consider removing\")\n",
    "        cat_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        for col in cat_cols:\n",
    "            self.results['metadata']['stats'][col] = {\n",
    "                'count': self.df[col].count(), 'unique': self.df[col].nunique(),\n",
    "                'top': self.df[col].mode().iloc[0] if not self.df[col].empty else None,\n",
    "                'freq': self.df[col].value_counts().iloc[0] if not self.df[col].empty else None\n",
    "            }\n",
    "        return self.results['metadata']['stats']\n",
    "\n",
    "    def analyze_missing_values(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze missing values.\"\"\"\n",
    "        logger.info(\"Analyzing missing values...\")\n",
    "        missing = self.df.isnull().sum()\n",
    "        missing_pct = (missing / len(self.df)) * 100\n",
    "        self.results['missing_values'] = {'count': missing.to_dict(), 'percentage': missing_pct.to_dict()}\n",
    "        fig = px.imshow(self.df.isnull(), title=\"Missing Values Heatmap\")\n",
    "        self._save_plot(fig, 'missing_values_heatmap')\n",
    "        return self.results['missing_values']\n",
    "\n",
    "    def detect_outliers(self, threshold: float = 3.5) -> Dict[str, Any]:\n",
    "        \"\"\"Detect outliers using Z-score and IQR.\"\"\"\n",
    "        logger.info(\"Detecting outliers...\")\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        outliers = {}\n",
    "        for col in numeric_cols:\n",
    "            data = self.df[col].dropna()\n",
    "            if len(data) < 10:\n",
    "                logger.warning(f\"Not enough data for {col} (n={len(data)})\")\n",
    "                continue\n",
    "            z_scores = np.abs(stats.zscore(data))\n",
    "            z_outliers = data[z_scores > threshold]\n",
    "            q1 = data.quantile(0.25)\n",
    "            q3 = data.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            iqr_outliers = data[(data < (q1 - 1.5*iqr)) | (data > (q3 + 1.5*iqr))]\n",
    "            outliers[col] = {\n",
    "                'z_score': {'count': len(z_outliers), 'indices': z_outliers.index.tolist(), 'values': z_outliers.values.tolist()},\n",
    "                'iqr': {'count': len(iqr_outliers), 'indices': iqr_outliers.index.tolist(), 'values': iqr_outliers.values.tolist()}\n",
    "            }\n",
    "            fig = px.box(self.df, y=col, title=f\"Outliers in {col}\")\n",
    "            self._save_plot(fig, f'outliers_boxplot_{col}')\n",
    "        self.results['outliers'] = outliers\n",
    "        return outliers\n",
    "\n",
    "    def analyze_distributions(self) -> None:\n",
    "        \"\"\"Analyze distributions of numeric columns.\"\"\"\n",
    "        logger.info(\"Analyzing distributions...\")\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            data = self.df[col].dropna()\n",
    "            if len(data) < 5:\n",
    "                continue\n",
    "            fig = px.histogram(self.df, x=col, nbins=30, title=f\"Distribution of {col}\", marginal=\"rug\")\n",
    "            self._save_plot(fig, f'distribution_{col}')\n",
    "            stat, p = stats.shapiro(data)\n",
    "            self.results['metadata']['stats'][col].update({\n",
    "                'normality_test': {'shapiro_stat': float(stat), 'shapiro_p': float(p), 'is_normal': bool(p > 0.05)}\n",
    "            })\n",
    "\n",
    "    def analyze_temporal_trends(self) -> None:\n",
    "        \"\"\"Analyze temporal trends.\"\"\"\n",
    "        logger.info(\"Analyzing temporal trends...\")\n",
    "        if 'TIME_PERIOD' not in self.df.columns:\n",
    "            logger.warning(\"Missing TIME_PERIOD column\")\n",
    "            return\n",
    "        self.df['year'] = self.df['TIME_PERIOD'].dt.year\n",
    "        yearly_stats = self.df.groupby('year')['OBS_VALUE'].agg(['count', 'mean', 'median', 'std', 'min', 'max'])\n",
    "        self.results['trends']['yearly_stats'] = yearly_stats.to_dict()\n",
    "        fig = px.line(yearly_stats.reset_index(), x='year', y='mean', error_y='std', title='Mean FDI by Year with Confidence Interval')\n",
    "        self._save_plot(fig, 'yearly_trends')\n",
    "\n",
    "    def analyze_correlations(self) -> None:\n",
    "        \"\"\"Analyze correlations between numeric columns.\"\"\"\n",
    "        logger.info(\"Analyzing correlations...\")\n",
    "        numeric_df = self.df.select_dtypes(include=[np.number]).dropna(axis=1, how='all')\n",
    "        numeric_df = numeric_df.loc[:, numeric_df.std() > 0]\n",
    "        if len(numeric_df.columns) < 2:\n",
    "            logger.warning(\"Not enough numeric columns for correlation analysis\")\n",
    "            return\n",
    "        corr_matrix = numeric_df.corr()\n",
    "        self.results['correlations']['matrix'] = corr_matrix.to_dict()\n",
    "        fig = px.imshow(corr_matrix, text_auto='.2f', title=\"Correlation Matrix\", color_continuous_scale='RdBu_r')\n",
    "        self._save_plot(fig, 'correlation_matrix')\n",
    "\n",
    "    def save_results(self) -> None:\n",
    "        \"\"\"Save analysis results to JSON.\"\"\"\n",
    "        def default_serializer(obj):\n",
    "            if isinstance(obj, (np.int64, np.float64)):\n",
    "                return float(obj)\n",
    "            if isinstance(obj, np.bool_):\n",
    "                return bool(obj)\n",
    "            return str(obj)\n",
    "        with open('eda_results.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.results, f, indent=4, ensure_ascii=False, default=default_serializer)\n",
    "        logger.info(\"Results saved to eda_results.json\")\n",
    "\n",
    "    def run_full_analysis(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run full EDA analysis.\"\"\"\n",
    "        if self.df.empty:\n",
    "            logger.warning(\"Empty DataFrame provided for EDA\")\n",
    "            return self.results\n",
    "        logger.info(\"Running full EDA analysis\")\n",
    "        self.describe_columns()\n",
    "        self.basic_statistics()\n",
    "        self.analyze_missing_values()\n",
    "        self.detect_outliers()\n",
    "        self.analyze_distributions()\n",
    "        self.analyze_temporal_trends()\n",
    "        self.analyze_correlations()\n",
    "        self.save_results()\n",
    "        logger.info(\"EDA analysis completed\")\n",
    "        return self.results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from data_loading import DataLoader\n",
    "    loader = DataLoader()\n",
    "    df = loader.load_data()\n",
    "    if df is not None and not df.empty:\n",
    "        analyzer = EDAAnalyzer(df)\n",
    "        results = analyzer.run_full_analysis()\n",
    "        print(\"\\n=== SUMMARY RESULTS ===\")\n",
    "        print(f\"Rows analyzed: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d827ce-eb0d-4d9b-aa1e-40e65710268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import logging\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Dict, Any\n",
    "import yaml\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('data_processing.log'), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "    'window_sizes': {'mean': 3, 'std': 3, 'min': 5, 'max': 5},\n",
    "    'output_dir': 'processed_data',\n",
    "    'fillna_methods': {'OBS_VALUE': 'interpolate', 'rolling_mean': 'zero', 'yearly_change': 'zero', 'rolling_std': 'zero'},\n",
    "    'features_to_scale': ['OBS_VALUE', 'rolling_mean', 'yearly_change'],\n",
    "    'visualization': True,\n",
    "    'max_abs_value': 1e6,\n",
    "    'save_parquet': False,\n",
    "    'outlier_threshold': 3.0,\n",
    "    'save_stats': True,\n",
    "    'save_excel': False,\n",
    "    'drop_columns': ['OBS_FLAG', 'CONF_STATUS'],\n",
    "    'validation_rules': {'OBS_VALUE': {'min': -1000, 'max': 1000}, 'yearly_change': {'min': -100, 'max': 100}},\n",
    "    'drop_na': False\n",
    "}\n",
    "\n",
    "def load_config(config_path: str = 'config.yaml') -> Dict[str, Any]:\n",
    "    \"\"\"Load configuration from YAML file or use defaults.\"\"\"\n",
    "    try:\n",
    "        with open(config_path) as f:\n",
    "            config = yaml.safe_load(f) or {}\n",
    "            logger.info(f\"Loaded config from {config_path}\")\n",
    "            def deep_update(source, overrides):\n",
    "                for key, value in overrides.items():\n",
    "                    if isinstance(value, dict) and key in source:\n",
    "                        deep_update(source[key], value)\n",
    "                    else:\n",
    "                        source[key] = value\n",
    "                return source\n",
    "            return deep_update(DEFAULT_CONFIG.copy(), config)\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"Config file {config_path} not found, using defaults\")\n",
    "        return DEFAULT_CONFIG\n",
    "\n",
    "def setup_output_dir(output_dir: str) -> None:\n",
    "    \"\"\"Set up output directory structure.\"\"\"\n",
    "    os.makedirs(os.path.join(output_dir, 'plots'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'stats'), exist_ok=True)\n",
    "    logger.info(f\"Directory structure created in {output_dir}\")\n",
    "\n",
    "def handle_missing_values(df: pd.DataFrame, methods: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"Handle missing values according to specified methods.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col, method in methods.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        if method == 'interpolate':\n",
    "            df[col] = df[col].interpolate(method='linear').ffill().bfill()\n",
    "        elif method == 'median':\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        elif method == 'mean':\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        elif method == 'zero':\n",
    "            df[col] = df[col].fillna(0)\n",
    "        elif method == 'ffill':\n",
    "            df[col] = df[col].ffill()\n",
    "        elif method == 'bfill':\n",
    "            df[col] = df[col].bfill()\n",
    "    logger.info(f\"Missing values handled. Remaining NaN in OBS_VALUE: {df['OBS_VALUE'].isna().sum()}\")\n",
    "    return df\n",
    "\n",
    "def calculate_rolling_features(df: pd.DataFrame, window_sizes: Dict[str, int]) -> pd.DataFrame:\n",
    "    \"\"\"Calculate rolling features for each geo group.\"\"\"\n",
    "    df = df.copy()\n",
    "    window = window_sizes.get('mean', 3)\n",
    "    df['rolling_mean'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "    df['yearly_change'] = df.groupby('geo')['OBS_VALUE'].transform(\n",
    "        lambda x: x.pct_change(fill_method=None).replace([np.inf, -np.inf], np.nan) * 100)\n",
    "    df['yearly_change'] = df.groupby('geo')['yearly_change'].transform(\n",
    "        lambda x: x.fillna(x.median() if x.notna().any() else 0))\n",
    "    window = window_sizes.get('std', 3)\n",
    "    df['rolling_std'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "    window = window_sizes.get('min', 5)\n",
    "    df['rolling_min'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window, min_periods=1).min())\n",
    "    window = window_sizes.get('max', 5)\n",
    "    df['rolling_max'] = df.groupby('geo')['OBS_VALUE'].transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "    logger.info(\"Rolling features calculated\")\n",
    "    return df\n",
    "\n",
    "def validate_data(df: pd.DataFrame, rules: Dict[str, Dict[str, float]]) -> bool:\n",
    "    \"\"\"Validate data against rules.\"\"\"\n",
    "    is_valid = True\n",
    "    for col, rule in rules.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        min_val = rule.get('min')\n",
    "        max_val = rule.get('max')\n",
    "        if min_val is not None and (df[col] < min_val).any():\n",
    "            logger.warning(f\"Found values below {min_val} in {col}\")\n",
    "            is_valid = False\n",
    "        if max_val is not None and (df[col] > max_val).any():\n",
    "            logger.warning(f\"Found values above {max_val} in {col}\")\n",
    "            is_valid = False\n",
    "    return is_valid\n",
    "\n",
    "def create_visualizations(df: pd.DataFrame, output_dir: str) -> None:\n",
    "    \"\"\"Create and save visualizations.\"\"\"\n",
    "    fig = px.histogram(df, x='OBS_VALUE', nbins=30, title='Distribution of OBS_VALUE')\n",
    "    fig.write_html(os.path.join(output_dir, 'plots', 'data_histogram.html'))\n",
    "    fig.show()\n",
    "    fig = px.box(df, y=['OBS_VALUE', 'rolling_mean', 'yearly_change'], title='Boxplots of Key Features')\n",
    "    fig.write_html(os.path.join(output_dir, 'plots', 'data_boxplot.html'))\n",
    "    fig.show()\n",
    "    sample_countries = df['geo'].unique()[:5]\n",
    "    fig = px.line(df[df['geo'].isin(sample_countries)], x='TIME_PERIOD', y='OBS_VALUE', color='geo', title='Sample Country Time Series')\n",
    "    fig.write_html(os.path.join(output_dir, 'plots', 'sample_time_series.html'))\n",
    "    fig.show()\n",
    "    corr_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "    fig = px.imshow(corr_matrix, text_auto='.2f', title='Correlation Matrix', color_continuous_scale='RdBu_r')\n",
    "    fig.write_html(os.path.join(output_dir, 'plots', 'correlation_matrix.html'))\n",
    "    fig.show()\n",
    "    logger.info(f\"Visualizations saved and displayed from {output_dir}/plots\")\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess data according to config.\"\"\"\n",
    "    logger.info(f\"Starting preprocessing. Initial shape: {df.shape}\")\n",
    "    if df.empty:\n",
    "        logger.warning(\"Empty DataFrame provided for preprocessing\")\n",
    "        return df\n",
    "    df = df.drop(columns=config.get('drop_columns', []), errors='ignore')\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    df[numeric_cols] = df[numeric_cols].clip(upper=config['max_abs_value'], lower=-config['max_abs_value'])\n",
    "    logger.info(f\"NaN in OBS_VALUE before handling: {df['OBS_VALUE'].isna().sum()}\")\n",
    "    df = handle_missing_values(df, config.get('fillna_methods', {}))\n",
    "    df = calculate_rolling_features(df, config.get('window_sizes', {}))\n",
    "    if 'TIME_PERIOD' in df.columns:\n",
    "        df['year'] = df['TIME_PERIOD'].dt.year\n",
    "        df['quarter'] = df['TIME_PERIOD'].dt.quarter\n",
    "    scaler = StandardScaler()\n",
    "    features_to_scale = [f for f in config.get('features_to_scale', []) if f in df.columns]\n",
    "    if features_to_scale:\n",
    "        df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "    df = df.dropna(how='all', axis=1)  # Удаление полностью пустых столбцов\n",
    "    logger.info(f\"Preprocessing completed. Final shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def save_processed_data(df: pd.DataFrame, output_dir: str, config: Dict[str, Any]) -> None:\n",
    "    \"\"\"Save processed data and statistics.\"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(\"No data to save\")\n",
    "        return\n",
    "    csv_path = os.path.join(output_dir, 'processed_data.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    logger.info(f\"Data saved to CSV: {csv_path}\")\n",
    "    if config.get('save_stats', True):\n",
    "        stats = {\n",
    "            'summary_stats': df.describe().to_dict(),\n",
    "            'missing_values': df.isnull().sum().to_dict(),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        with open(os.path.join(output_dir, 'stats', 'data_statistics.json'), 'w') as f:\n",
    "            json.dump(stats, f, indent=2, default=str)\n",
    "        logger.info(f\"Stats saved in {output_dir}/stats\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main preprocessing pipeline.\"\"\"\n",
    "    logger.info(\"Starting preprocessing pipeline\")\n",
    "    config = load_config()\n",
    "    setup_output_dir(config['output_dir'])\n",
    "    from data_loading import load_data\n",
    "    df = load_data()\n",
    "    if df is not None and not df.empty:\n",
    "        df_processed = preprocess_data(df, config)\n",
    "        if config['visualization']:\n",
    "            create_visualizations(df_processed, config['output_dir'])\n",
    "        save_processed_data(df_processed, config['output_dir'], config)\n",
    "        return df_processed\n",
    "    logger.warning(\"No data processed\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_result = main()\n",
    "    if not df_result.empty:\n",
    "        print(\"\\nPreprocessing Result: Rows processed =\", len(df_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce3ef3-4d81-43b5-8f4d-0b13727a4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import plotly.express as px\n",
    "import logging\n",
    "import pickle\n",
    "from typing import Dict, Any\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('clustering.log'), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "    'max_clusters': 10,\n",
    "    'random_state': 42,\n",
    "    'file_path': 'estat_tec00107_filtered_en.csv',\n",
    "    'fill_method': 'median',\n",
    "    'dbscan_eps': 0.5,\n",
    "    'dbscan_min_samples': 5,\n",
    "    'n_runs': 5  # Для кросс-валидации\n",
    "}\n",
    "\n",
    "def find_optimal_clusters(X_scaled: np.ndarray, max_clusters: int, random_state: int) -> int:\n",
    "    \"\"\"Find optimal number of clusters using silhouette score.\"\"\"\n",
    "    silhouette_scores = []\n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        if len(set(labels)) > 1:\n",
    "            silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
    "        else:\n",
    "            silhouette_scores.append(0)\n",
    "    optimal_k = np.argmax(silhouette_scores) + 2 if silhouette_scores else 2\n",
    "    logger.info(f\"Optimal number of clusters: {optimal_k}\")\n",
    "    return optimal_k\n",
    "\n",
    "def cluster_countries(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Cluster countries based on FDI features.\"\"\"\n",
    "    logger.info(\"Starting clustering\")\n",
    "    if df.empty:\n",
    "        logger.warning(\"Empty DataFrame provided for clustering\")\n",
    "        return pd.DataFrame(columns=['geo', 'fdi_mean', 'fdi_std', 'change_mean', 'cluster'])\n",
    "    \n",
    "    df_agg = df.groupby('geo').agg({\n",
    "        'OBS_VALUE': ['mean', 'std'],\n",
    "        'yearly_change': 'mean'\n",
    "    }).reset_index()\n",
    "    df_agg.columns = ['geo', 'fdi_mean', 'fdi_std', 'change_mean']\n",
    "    df_agg.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    initial_rows = len(df_agg)\n",
    "    if config.get('fill_method') == 'median':\n",
    "        df_agg.fillna({\n",
    "            'fdi_mean': df_agg['fdi_mean'].median(),\n",
    "            'change_mean': df_agg['change_mean'].median(),\n",
    "            'fdi_std': df_agg['fdi_std'].median()\n",
    "        }, inplace=True)\n",
    "    elif config.get('fill_method') == 'mean':\n",
    "        df_agg.fillna({\n",
    "            'fdi_mean': df_agg['fdi_mean'].mean(),\n",
    "            'change_mean': df_agg['change_mean'].mean(),\n",
    "            'fdi_std': df_agg['fdi_std'].mean()\n",
    "        }, inplace=True)\n",
    "    df_agg.dropna(subset=['fdi_mean', 'change_mean'], inplace=True)\n",
    "    logger.info(f\"After processing NaN, {len(df_agg)} rows remain (initially {initial_rows})\")\n",
    "    \n",
    "    if df_agg.empty:\n",
    "        logger.warning(\"No data available for clustering after preprocessing\")\n",
    "        return pd.DataFrame(columns=['geo', 'fdi_mean', 'fdi_std', 'change_mean', 'cluster'])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df_agg[['fdi_mean', 'change_mean']])\n",
    "    \n",
    "    methods = {\n",
    "        'KMeans': KMeans(n_clusters=find_optimal_clusters(X_scaled, config['max_clusters'], config['random_state']), random_state=config['random_state']),\n",
    "        'DBSCAN': DBSCAN(eps=config['dbscan_eps'], min_samples=config['dbscan_min_samples']),\n",
    "        'Agglomerative': AgglomerativeClustering(n_clusters=3),\n",
    "        'Spectral': SpectralClustering(n_clusters=3, random_state=config['random_state']),\n",
    "        'GMM': GaussianMixture(n_components=3, random_state=config['random_state'])\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in methods.items():\n",
    "        labels_list = []\n",
    "        for _ in range(config['n_runs']):\n",
    "            if name == 'GMM':\n",
    "                labels = model.fit_predict(X_scaled)\n",
    "            else:\n",
    "                labels = model.fit_predict(X_scaled)\n",
    "            labels_list.append(labels)\n",
    "        # Majority voting for stability\n",
    "        label_matrix = np.array(labels_list).T\n",
    "        final_labels = [np.bincount(row[row >= 0]).argmax() if np.any(row >= 0) else -1 for row in label_matrix]\n",
    "        results[name] = final_labels\n",
    "        if len(set(final_labels)) > 1:\n",
    "            silhouette = silhouette_score(X_scaled, final_labels)\n",
    "            davies_bouldin = davies_bouldin_score(X_scaled, final_labels)\n",
    "            logger.info(f\"{name}: {len(set(final_labels))} clusters, Silhouette={silhouette:.3f}, Davies-Bouldin={davies_bouldin:.3f}\")\n",
    "    \n",
    "    label_matrix = np.array([results[name] for name in results]).T\n",
    "    ensemble_labels = [np.bincount(row[row >= 0]).argmax() if np.any(row >= 0) else -1 for row in label_matrix]\n",
    "    df_agg['cluster'] = ensemble_labels\n",
    "    if len(set(ensemble_labels)) > 1:\n",
    "        ensemble_silhouette = silhouette_score(X_scaled, ensemble_labels)\n",
    "        logger.info(f\"Ensemble Silhouette Score: {ensemble_silhouette:.3f}\")\n",
    "    \n",
    "    fig = px.scatter(df_agg, x='fdi_mean', y='change_mean', color='cluster', hover_data=['geo'], title='Country Clustering')\n",
    "    fig.write_html('clusters.html')\n",
    "    fig.show()\n",
    "    logger.info(\"Clustering visualization saved and displayed\")\n",
    "    \n",
    "    for name, model in methods.items():\n",
    "        with open(f'{name}_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    logger.info(\"Models saved\")\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from data_loading import load_data\n",
    "    from data_preprocessing import preprocess_data\n",
    "    config = DEFAULT_CONFIG\n",
    "    df = load_data(config['file_path'])\n",
    "    df_processed = preprocess_data(df, config)\n",
    "    result = cluster_countries(df_processed, config)\n",
    "    if not result.empty:\n",
    "        print(\"\\nClustering Result: Rows clustered =\", len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702967a-10ae-475b-88ac-cabad5268f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('time_series_analysis.log'), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def prepare_data(df: pd.DataFrame, country: str) -> pd.DataFrame:\n",
    "    \"\"\"Prepare time series data for a specific country.\"\"\"\n",
    "    df = df[df['geo'] == country].copy()\n",
    "    if df.empty:\n",
    "        logger.warning(f\"No data available for country: {country}\")\n",
    "        return df\n",
    "    df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])\n",
    "    df.set_index('TIME_PERIOD', inplace=True)\n",
    "    df = df.sort_index()\n",
    "    df['OBS_VALUE'] = df['OBS_VALUE'].interpolate(method='time').fillna(method='ffill').fillna(method='bfill')\n",
    "    df['year'] = df.index.year\n",
    "    df['time_idx'] = np.arange(len(df))\n",
    "    logger.info(f\"Prepared data for {country} with {len(df)} rows\")\n",
    "    return df\n",
    "\n",
    "def arima_forecast(train: pd.Series, test: pd.Series, steps: int) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"ARIMA forecast.\"\"\"\n",
    "    try:\n",
    "        model = auto_arima(train, seasonal=False, trace=False)\n",
    "        model_fit = model.fit(train)\n",
    "        test_pred = model_fit.predict(n_periods=len(test))\n",
    "        future_pred = model_fit.predict(n_periods=steps)\n",
    "        return test_pred, future_pred\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ARIMA forecast failed: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def prophet_forecast(train: pd.DataFrame, test: pd.DataFrame, steps: int) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"Prophet forecast with yearly seasonality.\"\"\"\n",
    "    try:\n",
    "        df = train.reset_index().rename(columns={'TIME_PERIOD': 'ds', 'OBS_VALUE': 'y'})\n",
    "        model = Prophet(yearly_seasonality=True)\n",
    "        model.fit(df)\n",
    "        test_dates = pd.DataFrame({'ds': test.index})\n",
    "        future_dates = pd.date_range(start=train.index[-1], periods=steps + 1, freq='Y')[1:]\n",
    "        future_df = pd.DataFrame({'ds': pd.concat([pd.Series(test.index), pd.Series(future_dates)])})\n",
    "        forecast = model.predict(future_df)\n",
    "        test_pred = forecast['yhat'].iloc[:len(test)].values\n",
    "        future_pred = forecast['yhat'].iloc[len(test):].values\n",
    "        return test_pred, future_pred\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prophet forecast failed: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def rf_forecast(train: pd.DataFrame, test: pd.DataFrame, steps: int) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"Optimized Random Forest forecast.\"\"\"\n",
    "    try:\n",
    "        X_train = train[['year', 'time_idx']]\n",
    "        y_train = train['OBS_VALUE']\n",
    "        rf = RandomForestRegressor(random_state=42)\n",
    "        param_grid = {'n_estimators': [50, 100], 'max_depth': [10, None]}\n",
    "        grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        model = grid_search.best_estimator_\n",
    "        logger.info(f\"Best RF params: {grid_search.best_params_}\")\n",
    "        X_test = test[['year', 'time_idx']]\n",
    "        test_pred = model.predict(X_test)\n",
    "        future_dates = pd.date_range(start=train.index[-1], periods=steps + 1, freq='Y')[1:]\n",
    "        future_df = pd.DataFrame({'year': future_dates.year, 'time_idx': np.arange(len(train), len(train) + steps)})\n",
    "        future_pred = model.predict(future_df)\n",
    "        return test_pred, future_pred\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Random Forest forecast failed: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def sarima_forecast(train: pd.Series, test: pd.Series, steps: int) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"SARIMA forecast with seasonality.\"\"\"\n",
    "    try:\n",
    "        model = auto_arima(train, seasonal=True, m=1, trace=False)  # m=1 для годовых данных\n",
    "        order = model.order\n",
    "        seasonal_order = model.seasonal_order\n",
    "        sarima_model = SARIMAX(train, order=order, seasonal_order=seasonal_order)\n",
    "        model_fit = sarima_model.fit(disp=False)\n",
    "        test_pred = model_fit.forecast(steps=len(test))\n",
    "        future_pred = model_fit.forecast(steps=steps)\n",
    "        return test_pred, future_pred\n",
    "    except Exception as e:\n",
    "        logger.error(f\"SARIMA forecast failed: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def exp_smoothing_forecast(train: pd.Series, test: pd.Series, steps: int) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"Exponential Smoothing forecast.\"\"\"\n",
    "    try:\n",
    "        model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=1)\n",
    "        model_fit = model.fit()\n",
    "        test_pred = model_fit.forecast(len(test))\n",
    "        future_pred = model_fit.forecast(steps)\n",
    "        return test_pred, future_pred\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exponential Smoothing forecast failed: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def ensemble_forecast(predictions: Dict[str, np.ndarray], test: pd.Series) -> Tuple[Optional[np.ndarray], Optional[Dict[str, float]]]:\n",
    "    \"\"\"Ensemble forecast with weighted averaging.\"\"\"\n",
    "    valid_preds = {name: pred for name, pred in predictions.items() if pred is not None and len(pred) == len(test)}\n",
    "    if not valid_preds:\n",
    "        logger.warning(\"No valid predictions for ensemble\")\n",
    "        return None, None\n",
    "    weights = {}\n",
    "    total_mae = 0\n",
    "    for name, pred in valid_preds.items():\n",
    "        try:\n",
    "            mae = mean_absolute_error(test, pred)\n",
    "            weights[name] = 1 / (mae + 1e-6)\n",
    "            total_mae += weights[name]\n",
    "            logger.info(f\"{name} MAE: {mae:.3f}, Weight: {weights[name]:.3f}\")\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Error computing MAE for {name}: {str(e)}\")\n",
    "            return None, None\n",
    "    weights = {name: w / total_mae for name, w in weights.items()}\n",
    "    ensemble = np.zeros(len(test))\n",
    "    for name, pred in valid_preds.items():\n",
    "        ensemble += weights[name] * pred\n",
    "    return ensemble, weights\n",
    "\n",
    "def forecast_fdi(df: pd.DataFrame, country: str = 'Germany', steps: int = 3) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Forecast FDI using multiple models and ensemble.\"\"\"\n",
    "    logger.info(f\"Time series analysis for {country}\")\n",
    "    if df.empty:\n",
    "        logger.warning(\"Empty DataFrame provided for forecasting\")\n",
    "        return {model: {'mae': float('nan'), 'mae_std': float('nan')} for model in ['ARIMA', 'Prophet', 'RandomForest', 'SARIMA', 'ExpSmoothing', 'Ensemble']}\n",
    "    \n",
    "    data = prepare_data(df, country)\n",
    "    if data.empty or len(data) < 10:\n",
    "        logger.warning(f\"Insufficient data for {country} ({len(data)} rows)\")\n",
    "        return {model: {'mae': float('nan'), 'mae_std': float('nan')} for model in ['ARIMA', 'Prophet', 'RandomForest', 'SARIMA', 'ExpSmoothing', 'Ensemble']}\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=min(5, len(data) - 1))\n",
    "    models = {\n",
    "        'ARIMA': arima_forecast,\n",
    "        'Prophet': prophet_forecast,\n",
    "        'RandomForest': rf_forecast,\n",
    "        'SARIMA': sarima_forecast,\n",
    "        'ExpSmoothing': exp_smoothing_forecast\n",
    "    }\n",
    "    metrics = {}\n",
    "    errors = {}\n",
    "    all_predictions = {}\n",
    "    for train_idx, test_idx in tscv.split(data):\n",
    "        train, test = data.iloc[train_idx], data.iloc[test_idx]\n",
    "        predictions, future_predictions = {}, {}\n",
    "        for name, func in models.items():\n",
    "            test_pred, future_pred = func(\n",
    "                train['OBS_VALUE'] if name in ['ARIMA', 'SARIMA', 'ExpSmoothing'] else train,\n",
    "                test['OBS_VALUE'] if name in ['ARIMA', 'SARIMA', 'ExpSmoothing'] else test,\n",
    "                steps\n",
    "            )\n",
    "            predictions[name] = test_pred\n",
    "            future_predictions[name] = future_pred\n",
    "        \n",
    "        ensemble_test, weights = ensemble_forecast(predictions, test['OBS_VALUE'])\n",
    "        valid_future_preds = {k: v for k, v in future_predictions.items() if v is not None and len(v) == steps}\n",
    "        if valid_future_preds and weights:\n",
    "            ensemble_future = np.zeros(steps)\n",
    "            for name, pred in valid_future_preds.items():\n",
    "                ensemble_future += weights.get(name, 0) * pred\n",
    "        else:\n",
    "            ensemble_future = None\n",
    "        \n",
    "        for name, pred in predictions.items():\n",
    "            if pred is not None and len(pred) == len(test['OBS_VALUE']):\n",
    "                mae = mean_absolute_error(test['OBS_VALUE'], pred)\n",
    "                metrics.setdefault(name, []).append(mae)\n",
    "                errors.setdefault(name, []).append(test['OBS_VALUE'].values - pred)\n",
    "                all_predictions.setdefault(name, []).append(pred)\n",
    "        if ensemble_test is not None:\n",
    "            mae = mean_absolute_error(test['OBS_VALUE'], ensemble_test)\n",
    "            metrics.setdefault('Ensemble', []).append(mae)\n",
    "            errors.setdefault('Ensemble', []).append(test['OBS_VALUE'].values - ensemble_test)\n",
    "            all_predictions.setdefault('Ensemble', []).append(ensemble_test)\n",
    "    \n",
    "    avg_metrics = {name: {'mae': np.mean(maes), 'mae_std': np.std(maes)} for name, maes in metrics.items()}\n",
    "    \n",
    "    # Визуализация прогноза\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=data.index, y=data['OBS_VALUE'], mode='lines+markers', name='Historical'))\n",
    "    future_dates = pd.date_range(start=data.index[-1], periods=steps + 1, freq='Y')[1:]\n",
    "    if ensemble_future is not None:\n",
    "        fig.add_trace(go.Scatter(x=future_dates, y=ensemble_future, mode='lines+markers', name='Ensemble Forecast'))\n",
    "    fig.update_layout(title=f'FDI Forecast for {country}', xaxis_title='Date', yaxis_title='FDI Value')\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    fig.write_html(f'plots/forecast_{country}.html')\n",
    "    fig.show()\n",
    "    \n",
    "    # Визуализация ошибок\n",
    "    error_fig = go.Figure()\n",
    "    for name, error_list in errors.items():\n",
    "        error_flat = np.concatenate(error_list)\n",
    "        error_fig.add_trace(go.Box(y=error_flat, name=name))\n",
    "    error_fig.update_layout(title=f'Forecast Error Distribution for {country}', yaxis_title='Error')\n",
    "    error_fig.write_html(f'plots/forecast_errors_{country}.html')\n",
    "    error_fig.show()\n",
    "    \n",
    "    # Визуализация ошибок во времени для последнего фолда\n",
    "    time_error_fig = go.Figure()\n",
    "    for name, pred in predictions.items():\n",
    "        if pred is not None and len(pred) == len(test['OBS_VALUE']):\n",
    "            time_error_fig.add_trace(go.Scatter(x=test.index, y=test['OBS_VALUE'] - pred, mode='lines+markers', name=f'{name} Error'))\n",
    "    if ensemble_test is not None:\n",
    "        time_error_fig.add_trace(go.Scatter(x=test.index, y=test['OBS_VALUE'] - ensemble_test, mode='lines+markers', name='Ensemble Error'))\n",
    "    time_error_fig.update_layout(title=f'Forecast Errors Over Time for {country} (Last Fold)', xaxis_title='Date', yaxis_title='Error')\n",
    "    time_error_fig.write_html(f'plots/time_series_errors_{country}.html')\n",
    "    time_error_fig.show()\n",
    "    \n",
    "    # Визуализация фактических vs предсказанных значений для последнего фолда\n",
    "    pred_fig = go.Figure()\n",
    "    pred_fig.add_trace(go.Scatter(x=test.index, y=test['OBS_VALUE'], mode='lines+markers', name='Actual'))\n",
    "    for name, pred in predictions.items():\n",
    "        if pred is not None and len(pred) == len(test['OBS_VALUE']):\n",
    "            pred_fig.add_trace(go.Scatter(x=test.index, y=pred, mode='lines+markers', name=f'{name} Predicted'))\n",
    "    if ensemble_test is not None:\n",
    "        pred_fig.add_trace(go.Scatter(x=test.index, y=ensemble_test, mode='lines+markers', name='Ensemble Predicted'))\n",
    "    pred_fig.update_layout(title=f'Actual vs Predicted FDI for {country} (Last Fold)', xaxis_title='Date', yaxis_title='FDI Value')\n",
    "    pred_fig.write_html(f'plots/actual_vs_predicted_{country}.html')\n",
    "    pred_fig.show()\n",
    "    \n",
    "    # Визуализация метрик по фолдам\n",
    "    metrics_fig = go.Figure()\n",
    "    for name, mae_list in metrics.items():\n",
    "        metrics_fig.add_trace(go.Box(y=mae_list, name=name))\n",
    "    metrics_fig.update_layout(title=f'MAE Across Folds for {country}', yaxis_title='MAE')\n",
    "    metrics_fig.write_html(f'plots/mae_across_folds_{country}.html')\n",
    "    metrics_fig.show()\n",
    "    \n",
    "    logger.info(f\"Forecast plot, errors, and metrics saved and displayed\")\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    from data_loading import load_data\n",
    "    from data_preprocessing import preprocess_data\n",
    "    df = load_data()\n",
    "    df_clean = preprocess_data(df, {})\n",
    "    metrics = forecast_fdi(df_clean, 'Germany')\n",
    "    print(\"\\nForecast Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dde0e1-621f-4e93-8e22-10babb404f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import DBSCAN\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from typing import Dict, Any\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('anomaly_detection.log'), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AnomalyDetector:\n",
    "    def __init__(self, file_path: str = 'estat_tec00107_filtered_en.csv'):\n",
    "        \"\"\"Initialize AnomalyDetector.\"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.df: pd.DataFrame = None\n",
    "        self.results: Dict[str, Any] = {}\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        os.makedirs('results/models', exist_ok=True)\n",
    "        os.makedirs('results/plots', exist_ok=True)\n",
    "    \n",
    "    def load_data(self) -> bool:\n",
    "        \"\"\"Load data from file.\"\"\"\n",
    "        from data_loading import load_data\n",
    "        logger.info(\"Loading data...\")\n",
    "        self.df = load_data(self.file_path)\n",
    "        if self.df is None or self.df.empty:\n",
    "            logger.warning(\"No data loaded or empty DataFrame\")\n",
    "            return False\n",
    "        self.df = self.df.sort_values('TIME_PERIOD')\n",
    "        logger.info(f\"Loaded {len(self.df)} records\")\n",
    "        return True\n",
    "    \n",
    "    def detect_anomalies(self) -> bool:\n",
    "        \"\"\"Detect anomalies using multiple methods.\"\"\"\n",
    "        if not self.load_data():\n",
    "            return False\n",
    "        logger.info(\"Detecting anomalies...\")\n",
    "        if self.df['OBS_VALUE'].dropna().empty:\n",
    "            logger.warning(\"No valid OBS_VALUE data for anomaly detection\")\n",
    "            return False\n",
    "        X = self.df[['OBS_VALUE']].dropna().values\n",
    "        \n",
    "        methods = {\n",
    "            'IsolationForest': IsolationForest(contamination=0.05, random_state=42),\n",
    "            'LOF': LocalOutlierFactor(contamination=0.05, novelty=False),\n",
    "            'OneClassSVM': OneClassSVM(nu=0.05),\n",
    "            'DBSCAN': DBSCAN(eps=0.5, min_samples=5)\n",
    "        }\n",
    "        \n",
    "        for name, model in methods.items():\n",
    "            if name in ['LOF', 'DBSCAN']:\n",
    "                labels = model.fit_predict(X)\n",
    "            else:\n",
    "                labels = model.fit_predict(X)\n",
    "            self.df[f'{name}_anomaly'] = np.nan\n",
    "            self.df.loc[self.df['OBS_VALUE'].notna(), f'{name}_anomaly'] = np.where(labels == -1, 1, 0)\n",
    "            self.results[name] = {'model': model, 'anomaly_col': f'{name}_anomaly'}\n",
    "            logger.info(f\"{name} anomalies detected\")\n",
    "        \n",
    "        # Ensemble voting\n",
    "        anomaly_cols = [f'{name}_anomaly' for name in methods]\n",
    "        self.df['ensemble_anomaly'] = self.df[anomaly_cols].mode(axis=1)[0]\n",
    "        logger.info(\"Ensemble anomaly detection completed\")\n",
    "        \n",
    "        self._save_models()\n",
    "        return True\n",
    "    \n",
    "    def _save_models(self) -> None:\n",
    "        \"\"\"Save trained models.\"\"\"\n",
    "        for name, result in self.results.items():\n",
    "            model_path = f'results/models/{name}_model.pkl'\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(result['model'], f)\n",
    "        logger.info(\"Models saved\")\n",
    "    \n",
    "    def visualize_results(self) -> None:\n",
    "        \"\"\"Visualize anomaly detection results.\"\"\"\n",
    "        if not self.results:\n",
    "            logger.warning(\"No results to visualize\")\n",
    "            return\n",
    "        fig = px.scatter(self.df, x='TIME_PERIOD', y='OBS_VALUE', title='Detected Anomalies in FDI Data')\n",
    "        for name, result in self.results.items():\n",
    "            anomalies = self.df[self.df[result['anomaly_col']] == 1]\n",
    "            fig.add_scatter(x=anomalies['TIME_PERIOD'], y=anomalies['OBS_VALUE'], mode='markers', name=f'{name} Anomalies')\n",
    "        ensemble_anomalies = self.df[self.df['ensemble_anomaly'] == 1]\n",
    "        fig.add_scatter(x=ensemble_anomalies['TIME_PERIOD'], y=ensemble_anomalies['OBS_VALUE'], mode='markers', name='Ensemble Anomalies')\n",
    "        fig.write_html('results/plots/anomalies_detection.html')\n",
    "        fig.show()\n",
    "        logger.info(\"Anomaly plot saved and displayed\")\n",
    "        self.df.to_csv('results/anomalies_marked.csv', index=False)\n",
    "        logger.info(\"Data with anomaly labels saved\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main anomaly detection pipeline.\"\"\"\n",
    "    detector = AnomalyDetector()\n",
    "    if detector.load_data() and detector.detect_anomalies():\n",
    "        detector.visualize_results()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5bc9a-9181-403c-bbaf-a131f6867b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_validate, KFold, GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import plotly.express as px\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('feature_importance_analysis.log'), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FeatureImportanceAnalyzer:\n",
    "    def __init__(self, n_splits: int = 5, random_state: int = 42):\n",
    "        \"\"\"Initialize FeatureImportanceAnalyzer.\"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.best_params_ = None\n",
    "        self.importance_df_ = None\n",
    "        self.cv_results_ = None\n",
    "    \n",
    "    def load_and_preprocess(self, filepath: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and preprocess data.\"\"\"\n",
    "        from data_loading import load_data\n",
    "        from data_preprocessing import preprocess_data\n",
    "        logger.info(f\"Loading data from {filepath}...\")\n",
    "        df = load_data(filepath)\n",
    "        if df.empty:\n",
    "            logger.warning(\"Empty DataFrame after loading\")\n",
    "            return df\n",
    "        df = preprocess_data(df, {})\n",
    "        logger.info(f\"Preprocessed data: {len(df)} rows\")\n",
    "        return df\n",
    "    \n",
    "    def build_pipeline(self) -> Pipeline:\n",
    "        \"\"\"Build preprocessing and modeling pipeline.\"\"\"\n",
    "        numeric_features = ['year', 'rolling_mean', 'yearly_change']\n",
    "        categorical_features = ['geo']\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', SimpleImputer(strategy='median'), numeric_features),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "            ]\n",
    "        )\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', RandomForestRegressor(random_state=self.random_state))\n",
    "        ])\n",
    "        return pipeline\n",
    "    \n",
    "    def optimize_model(self, pipeline: Pipeline, X: pd.DataFrame, y: pd.Series) -> Pipeline:\n",
    "        \"\"\"Optimize model hyperparameters.\"\"\"\n",
    "        if X.empty or y.empty:\n",
    "            logger.warning(\"Empty data provided for model optimization\")\n",
    "            return pipeline\n",
    "        if len(X) < self.n_splits:\n",
    "            logger.warning(f\"Insufficient data ({len(X)} rows) for {self.n_splits}-fold CV. Using single fit.\")\n",
    "            pipeline.fit(X, y)\n",
    "            return pipeline\n",
    "        param_grid = {\n",
    "            'model__n_estimators': [50, 100, 200],\n",
    "            'model__max_depth': [None, 10, 20],\n",
    "            'model__min_samples_split': [2, 5]\n",
    "        }\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=self.n_splits, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "        grid_search.fit(X, y)\n",
    "        self.best_params_ = grid_search.best_params_\n",
    "        logger.info(f\"Best parameters: {self.best_params_}\")\n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    def evaluate_model(self, model: Pipeline, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate model using cross-validation.\"\"\"\n",
    "        if X.empty or y.empty:\n",
    "            logger.warning(\"Empty data provided for model evaluation\")\n",
    "            return {'test_MAE': [float('nan')], 'test_R2': [float('nan')]}\n",
    "        kfold = KFold(n_splits=min(self.n_splits, len(X)), shuffle=True, random_state=self.random_state)\n",
    "        scoring = {'MAE': make_scorer(mean_absolute_error, greater_is_better=False), 'R2': make_scorer(r2_score)}\n",
    "        cv_results = cross_validate(model, X, y, cv=kfold, scoring=scoring, n_jobs=-1, return_train_score=True)\n",
    "        self.cv_results_ = cv_results\n",
    "        return cv_results\n",
    "    \n",
    "    def compute_feature_importance(self, model: Pipeline, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"Compute feature importance with aggregation for categorical features.\"\"\"\n",
    "        if X.empty or y.empty:\n",
    "            logger.warning(\"Empty data provided for feature importance\")\n",
    "            return pd.DataFrame(columns=['feature', 'rf_importance', 'permutation_importance'])\n",
    "        \n",
    "        # Ограничение входных данных для пайплайна и permutation_importance\n",
    "        required_features = ['year', 'rolling_mean', 'yearly_change', 'geo']\n",
    "        X_filtered = X[required_features]\n",
    "        logger.info(f\"Using features for analysis: {required_features}\")\n",
    "        \n",
    "        preprocessor = model.named_steps['preprocessor']\n",
    "        X_transformed = preprocessor.transform(X_filtered)\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        logger.info(f\"Number of transformed features: {len(feature_names)}\")\n",
    "        \n",
    "        # Важность признаков из RandomForest\n",
    "        importances = model.named_steps['model'].feature_importances_\n",
    "        logger.info(f\"Length of RF importances: {len(importances)}\")\n",
    "        \n",
    "        # Агрегация важности для закодированных признаков 'geo'\n",
    "        rf_importance_dict = {'year': 0, 'rolling_mean': 0, 'yearly_change': 0, 'geo': 0}\n",
    "        for name, importance in zip(feature_names, importances):\n",
    "            if name.startswith('num__'):\n",
    "                feature = name.replace('num__', '')\n",
    "                rf_importance_dict[feature] = importance\n",
    "            elif name.startswith('cat__'):\n",
    "                rf_importance_dict['geo'] += importance\n",
    "        logger.info(f\"Aggregated RF importance: {rf_importance_dict}\")\n",
    "        \n",
    "        # Permutation importance на отфильтрованных исходных данных\n",
    "        result = permutation_importance(model, X_filtered, y, n_repeats=20, random_state=self.random_state, n_jobs=-1)\n",
    "        original_features = required_features\n",
    "        logger.info(f\"Length of permutation importances: {len(result.importances_mean)}\")\n",
    "        \n",
    "        # Проверка длины\n",
    "        if len(original_features) != len(result.importances_mean):\n",
    "            logger.error(f\"Length mismatch: original_features={len(original_features)}, permutation={len(result.importances_mean)}\")\n",
    "            raise ValueError(\"Mismatch between original features and permutation importance length\")\n",
    "        \n",
    "        try:\n",
    "            self.importance_df_ = pd.DataFrame({\n",
    "                'feature': original_features,\n",
    "                'rf_importance': [rf_importance_dict[f] for f in original_features],\n",
    "                'permutation_importance': result.importances_mean,\n",
    "                'permutation_std': result.importances_std\n",
    "            }).sort_values('rf_importance', ascending=False)\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Error creating DataFrame: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        return self.importance_df_\n",
    "    \n",
    "    def visualize_results(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Visualize feature importance and model metrics.\"\"\"\n",
    "        if self.importance_df_ is None or self.importance_df_.empty:\n",
    "            logger.warning(\"No feature importance data to visualize\")\n",
    "            return\n",
    "        \n",
    "        # Feature Importances (Random Forest)\n",
    "        fig = px.bar(self.importance_df_, x='feature', y='rf_importance', \n",
    "                     title='Feature Importances (Random Forest)')\n",
    "        fig.write_html('feature_importance_rf.html')\n",
    "        fig.show()\n",
    "        \n",
    "        # Feature Importance Comparison with error bars\n",
    "        fig = px.scatter(self.importance_df_, x='rf_importance', y='permutation_importance', \n",
    "                         text='feature', title='Feature Importance Comparison', \n",
    "                         error_y='permutation_std', hover_data=['feature'])\n",
    "        fig.write_html('feature_importance_comparison.html')\n",
    "        fig.show()\n",
    "        \n",
    "        # Distribution of FDI Intensity\n",
    "        fig = px.histogram(df, x='OBS_VALUE', nbins=30, title='Distribution of FDI Intensity')\n",
    "        fig.write_html('fdi_distribution.html')\n",
    "        fig.show()\n",
    "        \n",
    "        # Top 10 Countries by Average FDI Intensity\n",
    "        top_countries = df.groupby('geo')['OBS_VALUE'].mean().nlargest(10).reset_index()\n",
    "        fig = px.bar(top_countries, x='geo', y='OBS_VALUE', title='Top 10 Countries by Average FDI Intensity')\n",
    "        fig.write_html('top_countries.html')\n",
    "        fig.show()\n",
    "        \n",
    "        # Model Performance Metrics\n",
    "        if self.cv_results_:\n",
    "            metrics_df = pd.DataFrame({\n",
    "                'MAE': -self.cv_results_['test_MAE'],\n",
    "                'R2': self.cv_results_['test_R2']\n",
    "            })\n",
    "            fig = px.box(metrics_df, title='Model Performance Metrics Across Folds')\n",
    "            fig.write_html('model_metrics.html')\n",
    "            fig.show()\n",
    "        \n",
    "        logger.info(\"Visualizations saved and displayed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = FeatureImportanceAnalyzer()\n",
    "    df = analyzer.load_and_preprocess('estat_tec00107_filtered_en.csv')\n",
    "    if not df.empty:\n",
    "        pipeline = analyzer.build_pipeline()\n",
    "        pipeline = analyzer.optimize_model(pipeline, df, df['OBS_VALUE'])\n",
    "        cv_results = analyzer.evaluate_model(pipeline, df, df['OBS_VALUE'])\n",
    "        importance_df = analyzer.compute_feature_importance(pipeline, df, df['OBS_VALUE'])\n",
    "        analyzer.visualize_results(df)\n",
    "        print(\"\\nFeature Importance:\")\n",
    "        print(importance_df)\n",
    "        print(\"\\nModel Metrics:\")\n",
    "        print(f\"Test MAE: {-cv_results['test_MAE'].mean():.3f}\")\n",
    "        print(f\"Test R2: {cv_results['test_R2'].mean():.3f}\")\n",
    "    else:\n",
    "        logger.warning(\"No data for feature importance analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537a677-4559-4d8a-958d-3e5a218ab426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import logging\n",
    "from data_loading import load_data\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('investment_trends.log'), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def plot_investment_trends():\n",
    "    logger.info(\"Starting investment trends analysis\")\n",
    "    df = load_data('estat_tec00107_filtered_en.csv')\n",
    "    if df.empty:\n",
    "        logger.warning(\"No data loaded\")\n",
    "        return\n",
    "    \n",
    "    # Фильтрация по странам и последним 10 годам\n",
    "    countries = ['Germany', 'Sweden', 'Italy', 'Spain', 'France']\n",
    "    df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])\n",
    "    df = df[df['geo'].isin(countries) & (df['TIME_PERIOD'].dt.year >= 2014)]\n",
    "    \n",
    "    # Создание графика\n",
    "    fig = px.line(df, x='TIME_PERIOD', y='OBS_VALUE', color='geo', \n",
    "                  title='Объемы инвестиций для Германии, Швеции, Италии, Испании, Франции (2014–2023)',\n",
    "                  labels={'TIME_PERIOD': 'Год', 'OBS_VALUE': 'Значение инвестиций', 'geo': 'Страна'})\n",
    "    fig.write_html('investment_trends.html')\n",
    "    fig.show()\n",
    "    logger.info(\"Investment trends plot saved and displayed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plot_investment_trends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f110a-3e83-412e-a291-a5533b5d8067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import logging\n",
    "from data_loading import load_data\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('europe_vs_special.log'), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def compare_investments():\n",
    "    logger.info(\"Starting Europe vs Special countries comparison\")\n",
    "    df = load_data('estat_tec00107_filtered_en.csv')\n",
    "    if df.empty:\n",
    "        logger.warning(\"No data loaded\")\n",
    "        return\n",
    "    \n",
    "    # Фильтрация по странам и последним 10 годам\n",
    "    df['TIME_PERIOD'] = pd.to_datetime(df['TIME_PERIOD'])\n",
    "    df = df[df['TIME_PERIOD'].dt.year >= 2014]\n",
    "    \n",
    "    # Группы стран\n",
    "    europe = df[df['geo'] == 'European Union - 27 countries (from 2020)']\n",
    "    special = df[df['geo'].isin(['Cyprus', 'Luxembourg', 'Malta'])]\n",
    "    \n",
    "    # Средние значения\n",
    "    europe_mean = europe['OBS_VALUE'].mean()\n",
    "    special_mean = special.groupby('geo')['OBS_VALUE'].mean()\n",
    "    data = pd.DataFrame({\n",
    "        'Группа': ['Европа (EU-27)'] + special_mean.index.tolist(),\n",
    "        'Средние инвестиции': [europe_mean] + special_mean.values.tolist()\n",
    "    })\n",
    "    \n",
    "    # График\n",
    "    fig = px.bar(data, x='Группа', y='Средние инвестиции', \n",
    "                 title='Сравнение средних инвестиций: Европа vs Кипр, Люксембург, Мальта (2014–2023)',\n",
    "                 labels={'Средние инвестиции': 'Среднее значение инвестиций'})\n",
    "    fig.write_html('europe_vs_special.html')\n",
    "    fig.show()\n",
    "    logger.info(\"Comparison plot saved and displayed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_investments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181aa399-44f6-43ad-a4ae-8a7491c7e88d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
